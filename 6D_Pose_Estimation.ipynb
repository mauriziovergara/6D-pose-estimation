{"cells":[{"cell_type":"markdown","id":"ae3863ef","metadata":{"id":"ae3863ef"},"source":["# Install deps"]},{"cell_type":"code","execution_count":null,"id":"cf060277","metadata":{"executionInfo":{"elapsed":17832,"status":"ok","timestamp":1768726820420,"user":{"displayName":"Maurizio Pio Vergara","userId":"03891237400565096889"},"user_tz":-60},"id":"cf060277","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0ac736a-43c5-469d-ebfc-6f750c0e0e3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m740.4/740.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install -q ultralytics torch torchvision opencv-python-headless scipy plyfile pyyaml tqdm matplotlib pandas scikit-learn trimesh\n","\n","import torch, yaml, numpy as np, cv2\n","from pathlib import Path\n","import trimesh"]},{"cell_type":"markdown","id":"f1ac6aec","metadata":{"id":"f1ac6aec"},"source":["# Setup directories"]},{"cell_type":"code","execution_count":null,"id":"535aaf20","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2503,"status":"ok","timestamp":1768730261896,"user":{"displayName":"Maurizio Pio Vergara","userId":"03891237400565096889"},"user_tz":-60},"id":"535aaf20","outputId":"56a15f48-503b-4d5c-c7ed-057f86dba36f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","ZIP_ROOT = Path('/content/drive/My Drive/project_6')         # <- put your directory here\n","DRIVE_ROOT = Path('/content/drive/My Drive/pinhole_project') # <- put your directory here\n","LM_ZIP = ZIP_ROOT / 'Linemod_preprocessed.zip'\n","LM_ROOT = Path('/content/Linemod_preprocessed')\n","LM_DATA = LM_ROOT / 'data'\n","MODELS_DIR = LM_ROOT / 'models'\n","YOLO_RUN = DRIVE_ROOT / 'yolo' / 'linemod'\n","YOLO_BEST = YOLO_RUN / 'weights' / 'best.pt'\n","POSE_CKPT_DIR = DRIVE_ROOT / 'checkpoints'\n","POSE_BEST = POSE_CKPT_DIR / 'best_pose.pt'\n","POSE_LAST = POSE_CKPT_DIR / 'last_pose.pt'\n","EVAL_DIR = DRIVE_ROOT / 'eval_results'\n","VIZ_DIR = DRIVE_ROOT / 'visualizations'\n","for p in [POSE_CKPT_DIR, EVAL_DIR, VIZ_DIR, YOLO_RUN]:\n","    p.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"d34d77fe","metadata":{"id":"d34d77fe"},"outputs":[],"source":["from importlib.machinery import SourceFileLoader\n","import sys\n","import os\n","import json\n","import zipfile\n","import shutil\n","\n","sys.path.append(str(DRIVE_ROOT))\n","prep_mod = SourceFileLoader('prepare_data', str(\n","    DRIVE_ROOT / 'code' / 'prepare_data.py')).load_module()\n","train_yolo_mod = SourceFileLoader('train_yolo_linemod', str(\n","    DRIVE_ROOT / 'code' / 'train_yolo_linemod.py')).load_module()\n","infer_yolo_mod = SourceFileLoader('run_yolo_inference', str(\n","    DRIVE_ROOT / 'code' / 'run_yolo_inference.py')).load_module()\n","dataset_mod = SourceFileLoader('dataset', str(\n","    DRIVE_ROOT / 'code' / 'dataset.py')).load_module()\n","pose_model_mod = SourceFileLoader('pose_net_rgb_geometric', str(\n","    DRIVE_ROOT / 'code' / 'pose_net_rgb_geometric.py')).load_module()\n","train_pose_mod = SourceFileLoader('train_posenet_rgb', str(\n","    DRIVE_ROOT / 'code' / 'train_posenet_rgb.py')).load_module()\n","add_mod = SourceFileLoader('add_posenet', str(\n","    DRIVE_ROOT / 'code' / 'add_posenet.py')).load_module()\n","\n","\n","prepare_dataset = prep_mod.prepare_dataset\n","fine_tune_yolo = train_yolo_mod.fine_tune_yolo\n","run_yolo_inference = infer_yolo_mod.run_yolo_inference\n","train_pose_net = train_pose_mod.train_pose_net\n","run_add_eval = add_mod.run_add_eval\n","LineModDataset = dataset_mod.LineModDataset\n","PoseNetRGBGeometric = pose_model_mod.PoseNetRGBGeometric"]},{"cell_type":"markdown","id":"ca95487f","metadata":{"id":"ca95487f"},"source":["# Extract dataset"]},{"cell_type":"code","execution_count":null,"id":"cd8cc422","metadata":{"id":"cd8cc422","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768727034924,"user_tz":-60,"elapsed":175178,"user":{"displayName":"Maurizio Pio Vergara","userId":"03891237400565096889"}},"outputId":"f06ff50e-acd5-4f8a-8f68-fcd71c0bc806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting Linemod [3-4 minutes ...]\n"]}],"source":["if LM_ZIP.exists() and not LM_ROOT.exists():\n","    LM_ROOT.mkdir(parents=True, exist_ok=True)\n","    print('Extracting Linemod [3-4 minutes ...]')\n","    with zipfile.ZipFile(LM_ZIP, 'r') as z:\n","        z.extractall('/content')"]},{"cell_type":"markdown","id":"2e2a9458","metadata":{"id":"2e2a9458"},"source":["# Run prepare_dataset to build YOLO splits + linemod_multi.yaml"]},{"cell_type":"code","execution_count":null,"id":"8043f73f","metadata":{"id":"8043f73f"},"outputs":[],"source":["base_data = Path('/content/data')\n","base_data.mkdir(parents=True, exist_ok=True)\n","linemod_symlink = base_data / 'linemod'\n","if linemod_symlink.exists() or linemod_symlink.is_symlink():\n","    try:\n","        linemod_symlink.unlink()\n","    except Exception:\n","        shutil.rmtree(linemod_symlink, ignore_errors=True)\n","linemod_symlink.symlink_to(LM_DATA, target_is_directory=True)\n","\n","os.chdir(base_data)\n","prepare_dataset()\n","os.chdir('/content')\n","prep_summary = {\n","    'yaml_path': str(base_data / 'linemod_multi.yaml'),\n","    'yolo_prep_dir': str(base_data / 'linemod_yolo')\n","}\n","with open('/content/prep_summary.json', 'w') as f:\n","    json.dump(prep_summary, f, indent=2)\n","print('Prep done:', prep_summary)"]},{"cell_type":"markdown","id":"4a23f136","metadata":{"id":"4a23f136"},"source":["# Train YOLO"]},{"cell_type":"code","execution_count":null,"id":"0e9d4ab4","metadata":{"collapsed":true,"id":"0e9d4ab4"},"outputs":[],"source":["data_yaml = Path('/content/data/linemod_multi.yaml')\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","if YOLO_BEST.exists():\n","    print('Using existing YOLO weights:', YOLO_BEST)\n","else:\n","    _model, _results = fine_tune_yolo(\n","        pretrained_weights='yolov8n.pt',\n","        data_yaml=str(data_yaml),\n","        device=device,\n","        epochs=20,\n","        batch=32,\n","        imgsz=640,\n","        freeze=10,\n","        workers=2,\n","        project=str(YOLO_RUN.parent),\n","        name=YOLO_RUN.name\n","    )\n","    YOLO_BEST.parent.mkdir(parents=True, exist_ok=True)\n","print('YOLO best path:', YOLO_BEST)"]},{"cell_type":"markdown","id":"f2305d1e","metadata":{"id":"f2305d1e"},"source":["# YOLO inference"]},{"cell_type":"code","execution_count":null,"id":"6b05e7ec","metadata":{"id":"6b05e7ec"},"outputs":[],"source":["import yaml as _yaml\n","dataset_root_for_func = str(LM_ROOT)\n","full_pred_path = LM_DATA / 'yolo_predictions_full.yaml'\n","simple_pred_path = LM_DATA / 'yolo_predicted_boxes.yaml'\n","\n","preds = run_yolo_inference(\n","    yolo_weights_path=str(YOLO_BEST),\n","    dataset_root=dataset_root_for_func,\n","    output_path=str(full_pred_path),\n","    conf_threshold=0.25,\n","    device=device,\n",")\n","\n","with open(full_pred_path, 'r') as f:\n","    rich_preds = _yaml.safe_load(f) or {}\n","simple_preds = {}\n","for k, v in rich_preds.items():\n","    try:\n","        folder, sample = k.split('/')\n","        key = f\"{folder}/rgb/{int(sample):04d}.png\"\n","        simple_preds[key] = v['bbox']\n","    except Exception:\n","        continue\n","with open(simple_pred_path, 'w') as f:\n","    _yaml.safe_dump(simple_preds, f)\n","print('Saved simple boxes to', simple_pred_path)"]},{"cell_type":"markdown","id":"1pzA9_8ZHQnI","metadata":{"id":"1pzA9_8ZHQnI"},"source":["# PoseNet train preparation"]},{"cell_type":"code","execution_count":null,"id":"b7e92392","metadata":{"id":"b7e92392"},"outputs":[],"source":["pred_boxes_drive = DRIVE_ROOT / 'yolo_predicted_boxes.yaml'\n","pred_boxes_full = DRIVE_ROOT / 'yolo_predictions_full.yaml'\n","# pred_boxes_path = str(simple_pred_path) # if you do the YOLO train in one session\n","\n","pred_boxes_path = str(pred_boxes_drive)   # if you already have the .yaml\n","val_split = 0.2\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Pred boxes:', pred_boxes_path)\n","print('Val split:', val_split)\n","print('Device:', device)"]},{"cell_type":"markdown","source":["## PoseNet train RUN"],"metadata":{"id":"K4UqowElVLfK"},"id":"K4UqowElVLfK"},{"cell_type":"code","execution_count":null,"id":"8a6747cc","metadata":{"id":"8a6747cc","collapsed":true},"outputs":[],"source":["pose_model, pose_history = train_pose_net(\n","    root_dir=str(LM_DATA),\n","    models_dir=str(MODELS_DIR),\n","    pred_boxes_path=pred_boxes_path,\n","    epochs=110,\n","    batch_size=32,\n","    learning_rate=1e-4,\n","    weight_decay=1e-6,\n","    lambda_rot=1.0,\n","    lambda_trans=1.0,\n","    val_split=val_split,\n","    add_thresh=0.02,\n","    num_workers=2,\n","    seed=42,\n","    device=device,\n","    checkpoint_best=str(POSE_BEST),  # where to save best model\n","    checkpoint_last=str(POSE_LAST),  # where to save last model\n","    resume_from=str(POSE_LAST),      # None or str to resume from\n","    eval_only=False,                 # set to True to only evaluate\n","    scheduler_patience=5,            # for ReduceLROnPlateau (0 to disable)\n","    early_stop_patience=0,           # early stopping (0 to disable)\n","    optimizer_type='adamw',          # optimizer type: 'adam' or 'adamw'\n","    freeze_backbone_ratio_val=0.8,   # fraction of backbone layers to freeze\n","    unfreeze_epoch=100,              # epoch to unfreeze backbone\n","    rotation_loss_type='geodesic',\n",")"]},{"cell_type":"markdown","id":"2sKEUvM5HVQB","metadata":{"id":"2sKEUvM5HVQB"},"source":["# PoseNet evaluation & Best pose predictions.yaml"]},{"cell_type":"code","execution_count":null,"id":"ff6d1c4c","metadata":{"id":"ff6d1c4c"},"outputs":[],"source":["import pandas as pd\n","per_class, overall = run_add_eval(\n","    root_dir=str(LM_DATA),\n","    models_dir=str(MODELS_DIR),\n","    checkpoint=str(POSE_BEST),\n","    pred_boxes_path=pred_boxes_path,\n","    batch_size=32,\n","    val_split=val_split,\n","    num_workers=2,\n","    seed=42,\n","    device=device,\n","    output_csv=str(EVAL_DIR / 'add_results.csv'),\n","    save_predictions=str(EVAL_DIR / 'best_pose_predictions.yaml'),\n",")\n","\n","per_class_df = pd.DataFrame.from_dict(per_class, orient='index')\n","per_class_csv = EVAL_DIR / 'add_by_class.csv'\n","per_class_df.to_csv(per_class_csv)\n","metrics_json = EVAL_DIR / 'metrics.json'\n","with open(metrics_json, 'w') as f:\n","    json.dump({'overall': overall, 'per_class': per_class}, f, indent=2)\n","print('Saved:', per_class_csv, metrics_json)"]},{"cell_type":"markdown","source":["# PoseNet ADD-S evaluation (Symmetric)"],"metadata":{"id":"Gr8FH2hpyKBE"},"id":"Gr8FH2hpyKBE"},{"cell_type":"code","source":["adds_mod = SourceFileLoader('adds_posenet', str(\n","    DRIVE_ROOT / 'code' / 'adds_posenet.py')).load_module()\n","run_adds_eval = adds_mod.run_adds_eval\n","\n","per_class_adds, overall_adds = run_adds_eval(\n","    root_dir=str(LM_DATA),\n","    models_dir=str(MODELS_DIR),\n","    checkpoint=str(POSE_BEST),\n","    pred_boxes_path=pred_boxes_path,\n","    batch_size=32,\n","    val_split=val_split,\n","    num_workers=2,\n","    seed=42,\n","    device=device,\n","    output_csv=str(EVAL_DIR / 'adds_results_2classes.csv'),\n","    symmetric_only=True,\n",")\n","\n","per_class_adds_df = pd.DataFrame.from_dict(per_class_adds, orient='index')\n","per_class_adds_csv = EVAL_DIR / 'adds_by_class.csv'\n","per_class_adds_df.to_csv(per_class_adds_csv)\n","\n","metrics_adds_json = EVAL_DIR / 'metrics_adds.json'\n","with open(metrics_adds_json, 'w') as f:\n","    json.dump({'overall': overall_adds, 'per_class': per_class_adds}, f, indent=2)\n","\n","print(f'\\n✓ ADD-S evaluation completed!')\n","print(f'  - CSV summary: {EVAL_DIR / \"adds_results.csv\"}')\n","print(f'  - Per-class CSV: {per_class_adds_csv}')\n","print(f'  - Metrics JSON: {metrics_adds_json}')"],"metadata":{"id":"9dF7lMKwyLAP"},"id":"9dF7lMKwyLAP","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"_przQDlW_osP","metadata":{"id":"_przQDlW_osP"},"source":["# Visualize samples with bounding boxes and 3D axes"]},{"cell_type":"code","execution_count":null,"id":"aIVxzA45_qvZ","metadata":{"id":"aIVxzA45_qvZ","collapsed":true},"outputs":[],"source":["viz_mod = SourceFileLoader('visualize_samples', str(\n","    DRIVE_ROOT / 'code' / 'visualize_samples.py')).load_module()\n","visualize_random_samples = viz_mod.visualize_random_samples\n","\n","pose_pred_file = EVAL_DIR / 'best_pose_predictions.yaml'\n","\n","visualize_random_samples(\n","    root_dir=str(LM_DATA),\n","    output_dir=str(VIZ_DIR),\n","    axis_length=0.2,\n","    yolo_pred_path=str(\n","        pred_boxes_full) if pred_boxes_full.exists() else None,\n","    pose_pred_path=str(pose_pred_file) if pose_pred_file.exists() else None,\n",")\n","print(f'Visualizations saved to {VIZ_DIR}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ae3863ef","f1ac6aec","ca95487f","2e2a9458","4a23f136","f2305d1e"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}